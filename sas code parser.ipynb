{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c7a3e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "import os\n",
    "from pathlib import Path\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from dataclasses import asdict\n",
    "# Load .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69323e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class SASCodeChunk:\n",
    "    \"\"\"Structure to hold parsed SAS code chunks\"\"\"\n",
    "    chunk_type: str  # 'proc', 'macro', 'data_step', 'other'\n",
    "    code: str\n",
    "    comments: str\n",
    "    name: Optional[str] = None\n",
    "    line_start: int = 0\n",
    "    line_end: int = 0\n",
    "    filepath: str = \"\"  # NEW: Source file path\n",
    "    filename: str = \"\"  # NEW: Just the filename\n",
    "    explanation: Optional[str] = None\n",
    "\n",
    "def parse_sas_file(filepath: str) -> List[SASCodeChunk]:\n",
    "    \"\"\"\n",
    "    Parse a single SAS file and extract code chunks\n",
    "    Macros are treated as single chunks including all PROCs/DATA steps inside\n",
    "    \"\"\"\n",
    "    # Get filename for metadata\n",
    "    filename = Path(filepath).name\n",
    "    \n",
    "    # Regex patterns\n",
    "    proc_pattern = re.compile(r'^\\s*proc\\s+(\\w+)', re.IGNORECASE)\n",
    "    macro_pattern = re.compile(r'^\\s*%macro\\s+(\\w+)', re.IGNORECASE)\n",
    "    macro_end_pattern = re.compile(r'^\\s*%mend', re.IGNORECASE)\n",
    "    data_step_pattern = re.compile(r'^\\s*data\\s+', re.IGNORECASE)\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_comments = []\n",
    "    chunk_type = None\n",
    "    chunk_name = None\n",
    "    start_line = 0\n",
    "    inside_macro = False\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        # Collect comments\n",
    "        if line.strip().startswith('*') or line.strip().startswith('/*'):\n",
    "            current_comments.append(line.strip())\n",
    "            continue\n",
    "        \n",
    "        # Detect MACRO start\n",
    "        if macro_pattern.match(line):\n",
    "            if current_chunk:\n",
    "                chunks.append(SASCodeChunk(\n",
    "                    chunk_type=chunk_type or 'other',\n",
    "                    code=''.join(current_chunk),\n",
    "                    comments='\\n'.join(current_comments),\n",
    "                    name=chunk_name,\n",
    "                    line_start=start_line,\n",
    "                    line_end=i-1,\n",
    "                    filepath=filepath,\n",
    "                    filename=filename\n",
    "                ))\n",
    "            \n",
    "            chunk_type = 'macro'\n",
    "            chunk_name = macro_pattern.match(line).group(1)\n",
    "            current_chunk = [line]\n",
    "            start_line = i\n",
    "            current_comments = []\n",
    "            inside_macro = True\n",
    "            continue\n",
    "        \n",
    "        # Detect MACRO end\n",
    "        if macro_end_pattern.match(line):\n",
    "            if inside_macro:\n",
    "                current_chunk.append(line)\n",
    "                chunks.append(SASCodeChunk(\n",
    "                    chunk_type='macro',\n",
    "                    code=''.join(current_chunk),\n",
    "                    comments='\\n'.join(current_comments),\n",
    "                    name=chunk_name,\n",
    "                    line_start=start_line,\n",
    "                    line_end=i,\n",
    "                    filepath=filepath,\n",
    "                    filename=filename\n",
    "                ))\n",
    "                current_chunk = []\n",
    "                current_comments = []\n",
    "                chunk_type = None\n",
    "                chunk_name = None\n",
    "                inside_macro = False\n",
    "            continue\n",
    "        \n",
    "        # If inside macro, just add lines\n",
    "        if inside_macro:\n",
    "            current_chunk.append(line)\n",
    "            continue\n",
    "        \n",
    "        # Outside macro - detect PROC\n",
    "        if proc_pattern.match(line):\n",
    "            if current_chunk:\n",
    "                chunks.append(SASCodeChunk(\n",
    "                    chunk_type=chunk_type or 'other',\n",
    "                    code=''.join(current_chunk),\n",
    "                    comments='\\n'.join(current_comments),\n",
    "                    name=chunk_name,\n",
    "                    line_start=start_line,\n",
    "                    line_end=i-1,\n",
    "                    filepath=filepath,\n",
    "                    filename=filename\n",
    "                ))\n",
    "            \n",
    "            chunk_type = 'proc'\n",
    "            chunk_name = proc_pattern.match(line).group(1)\n",
    "            current_chunk = [line]\n",
    "            start_line = i\n",
    "            current_comments = []\n",
    "        \n",
    "        # Outside macro - detect DATA step\n",
    "        elif data_step_pattern.match(line):\n",
    "            if current_chunk:\n",
    "                chunks.append(SASCodeChunk(\n",
    "                    chunk_type=chunk_type or 'other',\n",
    "                    code=''.join(current_chunk),\n",
    "                    comments='\\n'.join(current_comments),\n",
    "                    name=chunk_name,\n",
    "                    line_start=start_line,\n",
    "                    line_end=i-1,\n",
    "                    filepath=filepath,\n",
    "                    filename=filename\n",
    "                ))\n",
    "            \n",
    "            chunk_type = 'data_step'\n",
    "            chunk_name = None\n",
    "            current_chunk = [line]\n",
    "            start_line = i\n",
    "            current_comments = []\n",
    "        \n",
    "        else:\n",
    "            if chunk_type:\n",
    "                current_chunk.append(line)\n",
    "        \n",
    "        # Detect chunk end for PROC/DATA\n",
    "        if not inside_macro and line.strip().lower() in ['run;', 'quit;']:\n",
    "            if current_chunk and chunk_type != 'macro':\n",
    "                chunks.append(SASCodeChunk(\n",
    "                    chunk_type=chunk_type or 'other',\n",
    "                    code=''.join(current_chunk),\n",
    "                    comments='\\n'.join(current_comments),\n",
    "                    name=chunk_name,\n",
    "                    line_start=start_line,\n",
    "                    line_end=i,\n",
    "                    filepath=filepath,\n",
    "                    filename=filename\n",
    "                ))\n",
    "                current_chunk = []\n",
    "                current_comments = []\n",
    "                chunk_type = None\n",
    "                chunk_name = None\n",
    "    \n",
    "    # Handle remaining chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(SASCodeChunk(\n",
    "            chunk_type=chunk_type or 'other',\n",
    "            code=''.join(current_chunk),\n",
    "            comments='\\n'.join(current_comments),\n",
    "            name=chunk_name,\n",
    "            line_start=start_line,\n",
    "            line_end=len(lines)-1,\n",
    "            filepath=filepath,\n",
    "            filename=filename\n",
    "        ))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49efb916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 SAS files\n",
      "\n",
      "\n",
      "############################################################\n",
      "FILE 1/4: d0_dm.sas\n",
      "############################################################\n",
      "Found 20 chunks in this file\n",
      "\n",
      "  Chunk 1/20: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 2/20: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 3/20: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 4/20: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 5/20: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 6/20: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 7/20: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 8/20: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 9/20: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 10/20: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 11/20: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 12/20: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 13/20: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 14/20: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 15/20: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 16/20: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 17/20: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 18/20: proc - casutil\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 19/20: proc - casutil\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 20/20: proc - datasets\n",
      "  ✓ Explanation added\n",
      "\n",
      "############################################################\n",
      "FILE 2/4: d1_ae.sas\n",
      "############################################################\n",
      "Found 18 chunks in this file\n",
      "\n",
      "  Chunk 1/18: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 2/18: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 3/18: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 4/18: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 5/18: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 6/18: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 7/18: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 8/18: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 9/18: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 10/18: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 11/18: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 12/18: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 13/18: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 14/18: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 15/18: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 16/18: proc - casutil\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 17/18: proc - casutil\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 18/18: proc - datasets\n",
      "  ✓ Explanation added\n",
      "\n",
      "############################################################\n",
      "FILE 3/4: d1_cm.sas\n",
      "############################################################\n",
      "Found 7 chunks in this file\n",
      "\n",
      "  Chunk 1/7: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 2/7: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 3/7: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 4/7: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 5/7: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 6/7: proc - sort\n",
      "  ✓ Explanation added\n",
      "\n",
      "  Chunk 7/7: data_step - None\n",
      "  ✓ Explanation added\n",
      "\n",
      "############################################################\n",
      "FILE 4/4: formats.sas\n",
      "############################################################\n",
      "Found 1 chunks in this file\n",
      "\n",
      "  Chunk 1/1: proc - format\n",
      "  ✓ Explanation added\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chunks_to_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     77\u001b[39m     all_chunks.extend(chunks)\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Save all chunks to JSON\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43mchunks_to_json\u001b[49m(all_chunks, \u001b[33m'\u001b[39m\u001b[33mchunk_explanation_12052025.json\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTotal: Processed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sas_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m files\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'chunks_to_json' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\"),\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    ")\n",
    "deployment_name = os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n",
    "\n",
    "def get_chunk_explanation(chunk: SASCodeChunk, full_file_content: str) -> str:\n",
    "    \"\"\"Get explanation for a code chunk using Azure OpenAI\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a SAS programming expert. Analyze this SAS code chunk and provide a clear explanation.\n",
    "\n",
    "FULL FILE CONTEXT:\n",
    "```sas\n",
    "{full_file_content}\n",
    "```\n",
    "\n",
    "SPECIFIC CHUNK TO EXPLAIN:\n",
    "Type: {chunk.chunk_type}\n",
    "Name: {chunk.name or 'N/A'}\n",
    "Lines: {chunk.line_start}-{chunk.line_end}\n",
    "\n",
    "Code:\n",
    "```sas\n",
    "{chunk.code}\n",
    "```\n",
    "\n",
    "Provide a concise explanation covering:\n",
    "1. What this chunk does\n",
    "2. Key operations or transformations\n",
    "3. How it fits in the overall context\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a SAS programming expert who explains code clearly and concisely.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Process all SAS files in folder\n",
    "folder_path = 'small_repo'\n",
    "all_chunks = []\n",
    "\n",
    "sas_files = list(Path(folder_path).rglob('*.sas'))\n",
    "print(f\"Found {len(sas_files)} SAS files\\n\")\n",
    "\n",
    "for file_idx, sas_file in enumerate(sas_files):\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"FILE {file_idx+1}/{len(sas_files)}: {sas_file.name}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    # Read full file content\n",
    "    with open(sas_file, 'r', encoding='utf-8') as f:\n",
    "        full_file_content = f.read()\n",
    "    \n",
    "    # Parse chunks\n",
    "    chunks = parse_sas_file(str(sas_file))\n",
    "    print(f\"Found {len(chunks)} chunks in this file\")\n",
    "    \n",
    "    # Process each chunk\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"\\n  Chunk {i+1}/{len(chunks)}: {chunk.chunk_type} - {chunk.name}\")\n",
    "        \n",
    "        try:\n",
    "            chunk.explanation = get_chunk_explanation(chunk, full_file_content)\n",
    "            print(f\"  ✓ Explanation added\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "            chunk.explanation = None\n",
    "    \n",
    "    all_chunks.extend(chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "540fa908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunks_to_json(chunks: List[SASCodeChunk], output_file: str = None):\n",
    "    \"\"\"\n",
    "    Convert SAS code chunks to JSON\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of SASCodeChunk objects\n",
    "        output_file: Optional filepath to save JSON (if None, returns string)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string if output_file is None, otherwise writes to file\n",
    "    \"\"\"\n",
    "    # Convert dataclasses to dictionaries\n",
    "    chunks_dict = [asdict(chunk) for chunk in chunks]\n",
    "    \n",
    "    if output_file:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks_dict, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved {len(chunks)} chunks to {output_file}\")\n",
    "    else:\n",
    "        return json.dumps(chunks_dict, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a29be76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 46 chunks to chunk_explanation_12052025.json\n",
      "\n",
      "\n",
      "Total: Processed 46 chunks from 4 files\n"
     ]
    }
   ],
   "source": [
    "# Save all chunks to JSON\n",
    "chunks_to_json(all_chunks, 'chunk_explanation_12052025.json')\n",
    "print(f\"\\n\\nTotal: Processed {len(all_chunks)} chunks from {len(sas_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd76915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sas-code-retreival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
